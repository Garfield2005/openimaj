<!DOCTYPE html><html xmlns:d="http://docbook.org/ns/docbook" xmlns:fo="http://www.w3.org/1999/XSL/Format">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   
      <title>Chapter&nbsp;12.&nbsp;Classification with Caltech 101</title>
      <link rel="stylesheet" type="text/css" href="css/apache-maven-fluido-1.3.0.min.css">
      <link rel="stylesheet" type="text/css" href="css/stylesheet.css">
      <link rel="stylesheet" type="text/css" href="fonts/fonts.css">
      <meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1">
      <link rel="home" href="index.html" title="The OpenIMAJ Tutorial">
      <link rel="up" href="pt06.html" title="Part&nbsp;VI.&nbsp;Machine Learning Fundamentals">
      <link rel="prev" href="pt06.html" title="Part&nbsp;VI.&nbsp;Machine Learning Fundamentals">
      <link rel="next" href="pt07.html" title="Part&nbsp;VII.&nbsp;Facial Analysis">
      <meta property="fb:admins" content="286108206">
      <meta property="og:type" content="website">
      <link rel="image_src" href="../images/OpenImaj-sq.png">
      <meta property="og:image" content="http://openimaj.org/images/OpenImaj-sq.png">
      <meta property="og:title" content="OpenIMAJ: Open Intelligent Multimedia Analysis">
      <meta property="og:url" content="http://www.openimaj.org">
      <meta property="og:description" content="OpenIMAJ is an award-winning set of libraries and tools for multimedia content analysis and content generation."><script type="text/javascript" src="./js/apache-maven-fluido-1.3.0.min.js"></script><script type="text/javascript">
	      var _gaq = _gaq || [];
	      _gaq.push(['_setAccount', 'UA-38338744-1']);
	      _gaq.push(['_trackPageview']);

	      (function() {
	        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
	        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
	        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
	      })();
	    </script></head>
   <body class="topBarEnabled">
      <div id="topbar" class="navbar navbar-fixed-top navbar-inverse">
         <div class="navbar-inner">
            <div class="container">
               <div class="nav-collapse"><a class="brand" href="../index.html" title="OpenIMAJ"><img src="images/logo-tiny.png" alt="OpenIMAJ"></a><ul class="nav">
                     <li class="dropdown"><a href="#" class="dropdown-toggle" data-toggle="dropdown">Overview <b class="caret"></b></a><ul class="dropdown-menu">
                           <li><a href="../index.html" title="Introduction">Introduction</a></li>
                           <li><a href="../sponsors.html" title="History &amp; Sponsors">History &amp; Sponsors</a></li>
                           <li><a href="../contact.html" title="Support &amp; Contacts">Support &amp; Contacts</a></li>
                           <li><a href="team.html" title="Team">Team</a></li>
                           <li><a href="http://blogs.ecs.soton.ac.uk/multimedia" title="The Blog">The Blog</a></li>
                           <li><a href="http://www.sf.net/p/openimaj/code" title="Source Code">Source Code</a></li>
                           <li class="dropdown-submenu"><a href="../#related" title="Related Projects">Related Projects</a><ul class="dropdown-menu">
                                 <li><a href="http://www.imageterrier.org/" title="ImageTerrier">ImageTerrier</a></li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li class="dropdown"><a href="#" class="dropdown-toggle" data-toggle="dropdown">Documentation <b class="caret"></b></a><ul class="dropdown-menu">
                           <li class="dropdown-submenu"><a href="../#gettingstarted" title="Getting Started">Getting Started</a><ul class="dropdown-menu">
                                 <li><a href="../tutorial-pdf.pdf" title="The Tutorial (PDF)">The Tutorial (PDF)</a></li>
                                 <li><a href="../tutorial/index.html" title="The Tutorial (HTML)">The Tutorial (HTML)</a></li>
                              </ul>
                           </li>
                           <li class="dropdown-submenu"><a href="../#using" title="Using OpenIMAJ">Using OpenIMAJ</a><ul class="dropdown-menu">
                                 <li><a href="../BuildFromSource.html" title="Building OpenIMAJ from Source">Building OpenIMAJ from Source</a></li>
                                 <li><a href="../UseLibrary.html" title="Using OpenIMAJ as a Library">Using OpenIMAJ as a Library</a></li>
                                 <li><a href="../Groovy.html" title="Using OpenIMAJ with Groovy">Using OpenIMAJ with Groovy</a></li>
                                 <li><a href="../tools.html" title="OpenIMAJ commandline tools introduction">OpenIMAJ commandline tools introduction</a></li>
                                 <li><a href="../flickrCrawler.html" title="The FlickrCrawler Tool">The FlickrCrawler Tool</a></li>
                                 <li><a href="../bibliography.html" title="Bibliography">Bibliography</a></li>
                              </ul>
                           </li>
                           <li><a href="../apidocs/index.html" title="API Reference">API Reference</a></li>
                           <li class="dropdown-submenu"><a href="#info" title="Project Information">Project Information</a><ul class="dropdown-menu">
                                 <li><a href="../plugin-management.html" title="Plugin Management">Plugin Management</a></li>
                                 <li><a href="../mail-lists.html" title="Mailing Lists">Mailing Lists</a></li>
                                 <li><a href="../integration.html" title="Continuous Integration">Continuous Integration</a></li>
                                 <li><a href="../license.html" title="Project License">Project License</a></li>
                                 <li><a href="../team-list.html" title="Project Team">Project Team</a></li>
                                 <li><a href="../source-repository.html" title="Source Repository">Source Repository</a></li>
                                 <li><a href="../index.html" title="About">About</a></li>
                                 <li><a href="../issue-tracking.html" title="Issue Tracking">Issue Tracking</a></li>
                                 <li><a href="../project-summary.html" title="Project Summary">Project Summary</a></li>
                                 <li><a href="../plugins.html" title="Project Plugins">Project Plugins</a></li>
                                 <li><a href="../dependency-convergence.html" title="Dependency Convergence">Dependency Convergence</a></li>
                                 <li><a href="../dependencies.html" title="Dependencies">Dependencies</a></li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li class="dropdown"><a href="#" class="dropdown-toggle" data-toggle="dropdown">Modules <b class="caret"></b></a><ul class="dropdown-menu">
                           <li><a href="/openimaj-maven-archetypes/index.html" title="OpenIMAJ Maven Archetypes">OpenIMAJ Maven Archetypes</a></li>
                           <li><a href="/openimaj-core-libs/index.html" title="OpenIMAJ Core Libraries">OpenIMAJ Core Libraries</a></li>
                           <li><a href="/openimaj-image/index.html" title="OpenIMAJ Image Processing Libraries">OpenIMAJ Image Processing Libraries</a></li>
                           <li><a href="/openimaj-video/index.html" title="OpenIMAJ Video Processing Libraries">OpenIMAJ Video Processing Libraries</a></li>
                           <li><a href="/openimaj-audio/index.html" title="OpenIMAJ Audio Processing Libraries">OpenIMAJ Audio Processing Libraries</a></li>
                           <li><a href="/openimaj-machine-learning/index.html" title="OpenIMAJ Machine Learning Subprojects">OpenIMAJ Machine Learning Subprojects</a></li>
                           <li><a href="/openimaj-text/index.html" title="OpenIMAJ Text Analysis Subprojects">OpenIMAJ Text Analysis Subprojects</a></li>
                           <li><a href="/thirdparty/index.html" title="OpenIMAJ Third Party Ported Libraries">OpenIMAJ Third Party Ported Libraries</a></li>
                           <li><a href="/openimaj-demos/index.html" title="OpenIMAJ Demos Subproject">OpenIMAJ Demos Subproject</a></li>
                           <li><a href="/openimaj-knowledge/index.html" title="OpenIMAJ Knowledge Representation and Reasoning Libraries">OpenIMAJ Knowledge Representation and Reasoning Libraries</a></li>
                           <li><a href="/test-resources/index.html" title="OpenIMAJ Unit Test Resources">OpenIMAJ Unit Test Resources</a></li>
                           <li><a href="/openimaj-tools/index.html" title="OpenIMAJ Tools">OpenIMAJ Tools</a></li>
                           <li><a href="/openimaj-hadoop/index.html" title="OpenIMAJ Hadoop Subproject">OpenIMAJ Hadoop Subproject</a></li>
                           <li><a href="/openimaj-storm/index.html" title="OpenIMAJ Storm Subproject">OpenIMAJ Storm Subproject</a></li>
                           <li><a href="/openimaj-web/index.html" title="OpenIMAJ web subproject">OpenIMAJ web subproject</a></li>
                           <li><a href="/openimaj-hardware/index.html" title="OpenIMAJ Hardware Subprojects">OpenIMAJ Hardware Subprojects</a></li>
                           <li><a href="/openimaj-content-libs/index.html" title="OpenIMAJ Content Creation Libraries">OpenIMAJ Content Creation Libraries</a></li>
                           <li><a href="/openimaj-ide-integration/index.html" title="OpenIMAJ IDE Integration Plugins">OpenIMAJ IDE Integration Plugins</a></li>
                           <li><a href="/openimaj-documentation/index.html" title="OpenIMAJ Documentation">OpenIMAJ Documentation</a></li>
                        </ul>
                     </li>
                  </ul>
                  <form id="search-form" action="http://www.google.com/search" method="get" class="navbar-search pull-right" name="search-form"><input value="" name="sitesearch" type="hidden"><input class="search-query" name="q" id="query" type="text"></form><script type="text/javascript" src="http://www.google.com/coop/cse/brand?form=search-form"></script><iframe src="http://www.facebook.com/plugins/like.php?href=http://www.openimaj.org/&amp;send=false&amp;layout=button_count&amp;show-faces=false&amp;action=like&amp;colorscheme=dark" scrolling="no" frameborder="0" style="border:none; width:80px; height:20px; margin-top: 10px;" class="pull-right"></iframe><script type="text/javascript" src="https://apis.google.com/js/plusone.js"></script><ul class="nav pull-right">
                     <li style="margin-top: 10px;">
                        <div class="g-plusone" data-href="http://www.openimaj.org/" data-size="medium" width="60px" align="right"></div>
                     </li>
                  </ul>
               </div>
            </div>
         </div>
      </div>
      <div class="container">
         <div id="banner">
            <div class="pull-left">
               <div id="bannerLeft">
                  <h2>OpenIMAJ</h2>
               </div>
            </div>
            <div class="pull-right"></div>
            <div class="clear">
               <hr>
            </div>
         </div>
         <div id="breadcrumbs">
            <ul class="breadcrumb">
               <li id="publishDate" class="pull-right">Last Published: 2015-02-25</li>
               <li class="divider pull-right">|</li>
               <li id="projectVersion" class="pull-right">Version: 1.4-SNAPSHOT</li>
            </ul>
         </div>
         <div id="nav-sub-block"><a class="hide" name="nav-sub-a"></a><ul id="nav-sub">
               <li class="first"><a accesskey="p" href="pt06.html"><span>Prev : Machine Learning Fundamentals</span></a></li>
               <li><a accesskey="h" href="index.html"><span>Contents</span></a></li>
               <li class="last"><a accesskey="n" href="pt07.html"><span>Next : Facial Analysis</span></a></li>
            </ul>
         </div>
         <div class="chapter" title="Chapter&nbsp;12.&nbsp;Classification with Caltech 101">
            <div class="titlepage">
               <div>
                  <div>
                     <h2 class="title"><a name="classification101"></a>Chapter&nbsp;12.&nbsp;Classification with Caltech 101
                     </h2>
                  </div>
               </div>
            </div>
            		
            		
            <p>
               		  In this tutorial, we&#8217;ll go through the steps required to build and
               		  evaluate a near state-of-the-art image classifier. Although for the
               		  purposes of this tutorial we&#8217;re using features extracted from images,
               		  everything you&#8217;ll learn about using classifiers can be applied to
               		  features extracted from other forms of media.
               		
            </p>
            		
            <p>
               		  To get started you&#8217;ll need a new class in an existing OpenIMAJ
               		  project, or a new project created with the archetype. The first thing
               		  we need is a dataset of images with which we&#8217;ll work. For this
               		  tutorial we&#8217;ll use a well known set of labelled images called the
               		  <a class="ulink" href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/" target="_top">Caltech
                  		  101 dataset</a>. The Caltech 101 dataset contains labelled images
               		  of 101 object classes together with a set of background images.
               		  OpenIMAJ has built in support for working with the Caltech 101
               		  dataset, and will even automatically download the dataset for you. To
               		  use it, enter the following code:
               		
            </p>
            		<pre class="programlisting">GroupedDataset&lt;String, VFSListDataset&lt;Record&lt;FImage&gt;&gt;, Record&lt;FImage&gt;&gt; allData = 
			Caltech101.getData(ImageUtilities.FIMAGE_READER);</pre>
            		<p>
               		  You&#8217;ll remember from the image datasets tutorial that
               		  <code class="literal">GroupedDataset</code>s are Java <code class="literal">Map</code>s
               		  with a few extra features. In this case, our
               		  <code class="literal">allData</code> object is a
               		  <code class="literal">GroupedDataset</code> with <code class="literal">String</code> keys
               		  and the values are lists (actually <code class="literal">VFSListDataset</code>s)
               		  of <code class="literal">Record</code> objects which are themselves typed on
               		  <code class="literal">FImage</code>s. The <code class="literal">Record</code> class holds
               		  metadata about each Caltech 101 image. <code class="literal">Record</code>s have
               		  a method called <code class="literal">getImage()</code> that will return the
               		  actual image in the format specified by the generic type of the
               		  <code class="literal">Record</code> (i.e. <code class="literal">FImage</code>).
               		
            </p>
            		
            <p>
               		  For this tutorial we&#8217;ll work with a subset of the classes in the
               		  dataset to minimise the time it takes our program to run. We can
               		  create a subset of groups in a <code class="literal">GroupedDataset</code> using
               		  the <code class="literal">GroupSampler</code> class:
               		
            </p>
            		<pre class="programlisting">GroupedDataset&lt;String, ListDataset&lt;Record&lt;FImage&gt;&gt;, Record&lt;FImage&gt;&gt; data = 
			GroupSampler.sample(allData, 5, false);</pre>
            		<p>
               		  This basically creates a new dataset called <code class="literal">data</code>
               		  from the first 5 classes in the <code class="literal">allData</code> dataset. To
               		  do an experimental evaluation with the dataset we need to create two
               		  sets of images: a <span class="strong"><strong>training</strong></span> set
               		  which we&#8217;ll use to learn the classifier, and a
               		  <span class="strong"><strong>testing</strong></span> set which we&#8217;ll evaluate
               		  the classifier with. The common approach with the Caltech 101 dataset
               		  is to choose a number of training and testing instances for each class
               		  of images. Programatically, this can be achieved using the
               		  <code class="literal">GroupedRandomSplitter</code> class:
               		
            </p>
            		<pre class="programlisting">GroupedRandomSplitter&lt;String, Record&lt;FImage&gt;&gt; splits = 
			new GroupedRandomSplitter&lt;String, Record&lt;FImage&gt;&gt;(data, 15, 0, 15);</pre>
            		<p>
               		  In this case, we&#8217;ve created a training dataset with 15 images per
               		  group, and 15 testing images per group. The zero in the constructor is
               		  the number of validation images which we won&#8217;t use in this tutorial.
               		  If you take a look at the <code class="literal">GroupedRandomSplitter</code>
               		  class you&#8217;ll see there are methods to get the training, validation and
               		  test datasets.
               		
            </p>
            		
            <p>
               		  Our next step is to consider how we&#8217;re going to extract suitable image
               		  features. For this tutorial we&#8217;re going to use a technique commonly
               		  known as the Pyramid Histogram of Words
               		  (<span class="strong"><strong>PHOW</strong></span>). PHOW is itself based on the
               		  idea of extracting <span class="strong"><strong>Dense SIFT</strong></span>
               		  features, quantising the SIFT features into
               		  <span class="strong"><strong>visual words</strong></span> and then building
               		  <span class="strong"><strong>spatial histograms</strong></span> of the visual
               		  word occurrences.
               		
            </p>
            		
            <p>
               		  The Dense SIFT features are just like the features you used in the
               		  <span class="quote">&#8220;<span class="quote">SIFT and feature matching</span>&#8221;</span> tutorial, but rather than
               		  extracting the features at interest points detected using a
               		  difference-of-Gaussian, the features are extracted on a regular grid
               		  across the image. The idea of a visual word is quite simple: rather
               		  than representing each SIFT feature by a 128 dimension feature vector,
               		  we represent it by an identifier. Similar features (i.e.&nbsp;those that
               		  have similar, but not necessarily the same, feature vectors) are
               		  assigned to have the same identifier. A common approach to assigning
               		  identifiers to features is to train a <span class="strong"><strong>vector
                     		  quantiser</strong></span> (just another fancy name for a type of
               		  <span class="emphasis"><em>classifier</em></span>) using k-means, just like we did in
               		  the <span class="quote">&#8220;<span class="quote">Introduction to clustering</span>&#8221;</span> tutorial. To build a
               		  histogram of visual words (often called a <span class="strong"><strong>Bag
                     		  of Visual Words</strong></span>), all we have to do is count up how many
               		  times each identifier appears in an image and store the values in a
               		  histogram. If we&#8217;re building spatial histograms, then the process is
               		  the same, but we effectively cut the image into blocks and compute the
               		  histogram for each block independently before concatenating the
               		  histograms from all the blocks into a larger histogram.
               		
            </p>
            		
            <p>
               		  To get started writing the code for the PHOW implementation, we first
               		  need to construct our Dense SIFT extractor - we&#8217;re actually going to
               		  construct two objects: a <code class="literal">DenseSIFT</code> object and a
               		  <code class="literal">PyramidDenseSIFT</code> object:
               		
            </p>
            		<pre class="programlisting">DenseSIFT dsift = new DenseSIFT(5, 7);
PyramidDenseSIFT&lt;FImage&gt; pdsift = new PyramidDenseSIFT&lt;FImage&gt;(dsift, 6f, 7);</pre>
            		<p>
               		  The <code class="literal">PyramidDenseSIFT</code> class takes a normal
               		  <code class="literal">DenseSIFT</code> instance and applies it to different
               		  sized windows on the regular sampling grid, although in this
               		  particular case we&#8217;re only using a single window size of 7 pixels.
               		
            </p>
            		
            <p>
               		  The next stage is to write some code to perform
               		  <span class="strong"><strong>K-Means</strong></span> clustering on a sample of
               		  SIFT features in order to build a <code class="literal">HardAssigner</code> that
               		  can assign features to identifiers. Let&#8217;s wrap up the code for this in
               		  a new method that takes as input a dataset and a
               		  <code class="literal">PyramidDenseSIFT</code> object:
               		
            </p>
            		<pre class="programlisting">static HardAssigner&lt;byte[], float[], IntFloatPair&gt; trainQuantiser(
	            Dataset&lt;Record&lt;FImage&gt;&gt; sample, PyramidDenseSIFT&lt;FImage&gt; pdsift)
{
    List&lt;LocalFeatureList&lt;ByteDSIFTKeypoint&gt;&gt; allkeys = new ArrayList&lt;LocalFeatureList&lt;ByteDSIFTKeypoint&gt;&gt;();

    for (Record&lt;FImage&gt; rec : sample) {
        FImage img = rec.getImage();

        pdsift.analyseImage(img);
        allkeys.add(pdsift.getByteKeypoints(0.005f));
    }

    if (allkeys.size() &gt; 10000)
        allkeys = allkeys.subList(0, 10000);

    ByteKMeans km = ByteKMeans.createKDTreeEnsemble(300);
    DataSource&lt;byte[]&gt; datasource = new LocalFeatureListDataSource&lt;ByteDSIFTKeypoint, byte[]&gt;(allkeys);
    ByteCentroidsResult result = km.cluster(datasource);

    return result.defaultHardAssigner();
}</pre>
            		<p>
               		  The above method extracts the first 10000 dense SIFT features from the
               		  images in the dataset, and then clusters them into 300 separate
               		  classes. The method then returns a <code class="literal">HardAssigner</code>
               		  which can be used to assign SIFT features to identifiers. To use this
               		  method, add the following to your main method after the
               		  <code class="literal">PyramidDenseSIFT</code> construction:
               		
            </p>
            		<pre class="programlisting">HardAssigner&lt;byte[], float[], IntFloatPair&gt; assigner = 
			trainQuantiser(GroupedUniformRandomisedSampler.sample(splits.getTrainingDataset(), 30), pdsift);</pre>
            		<p>
               		  Notice that we&#8217;ve used a
               		  <code class="literal">GroupedUniformRandomisedSampler</code> to get a random
               		  sample of 30 images across all the groups of the training set with
               		  which to train the quantiser. The next step is to write a
               		  <code class="literal">FeatureExtractor</code> implementation with which we can
               		  train our classifier:
               		
            </p>
            		<pre class="programlisting">static class PHOWExtractor implements FeatureExtractor&lt;DoubleFV, Record&lt;FImage&gt;&gt; {
    PyramidDenseSIFT&lt;FImage&gt; pdsift;
    HardAssigner&lt;byte[], float[], IntFloatPair&gt; assigner;

    public PHOWExtractor(PyramidDenseSIFT&lt;FImage&gt; pdsift, HardAssigner&lt;byte[], float[], IntFloatPair&gt; assigner)
    {
        this.pdsift = pdsift;
        this.assigner = assigner;
    }

    public DoubleFV extractFeature(Record&lt;FImage&gt; object) {
        FImage image = object.getImage();
        pdsift.analyseImage(image);

        BagOfVisualWords&lt;byte[]&gt; bovw = new BagOfVisualWords&lt;byte[]&gt;(assigner);

        BlockSpatialAggregator&lt;byte[], SparseIntFV&gt; spatial = new BlockSpatialAggregator&lt;byte[], SparseIntFV&gt;(
                bovw, 2, 2);

        return spatial.aggregate(pdsift.getByteKeypoints(0.015f), image.getBounds()).normaliseFV();
    }
}</pre>
            		<p>
               		  This class uses a <code class="literal">BlockSpatialAggregator</code> together
               		  with a <code class="literal">BagOfVisualWords</code> to compute 4 histograms
               		  across the image (by breaking the image into 2 both horizontally and
               		  vertically). The <code class="literal">BagOfVisualWords</code> uses the
               		  <code class="literal">HardAssigner</code> to assign each Dense SIFT feature to a
               		  visual word and the compute the histogram. The resultant spatial
               		  histograms are then appended together and normalised before being
               		  returned. Back in the main method of our code we can construct an
               		  instance of our PHOWExtractor:
               		
            </p>
            		<pre class="programlisting">FeatureExtractor&lt;DoubleFV, Record&lt;FImage&gt;&gt; extractor = new PHOWExtractor(pdsift, assigner);</pre>
            		<p>
               		  Now we&#8217;re ready to construct and train a classifier - we&#8217;ll use the
               		  linear classifier provided by the
               		  <code class="literal">LiblinearAnnotator</code> class:
               		
            </p>
            		<pre class="programlisting">LiblinearAnnotator&lt;Record&lt;FImage&gt;, String&gt; ann = new LiblinearAnnotator&lt;Record&lt;FImage&gt;, String&gt;(
		            extractor, Mode.MULTICLASS, SolverType.L2R_L2LOSS_SVC, 1.0, 0.00001);
ann.train(splits.getTrainingDataset());</pre>
            		<p>
               		  Finally, we can use the OpenIMAJ evaluation framework to perform an
               		  automated evaluation of our classifier&#8217;s accuracy for us:
               		
            </p>
            		<pre class="programlisting">ClassificationEvaluator&lt;CMResult&lt;String&gt;, String, Record&lt;FImage&gt;&gt; eval = 
			new ClassificationEvaluator&lt;CMResult&lt;String&gt;, String, Record&lt;FImage&gt;&gt;(
				ann, splits.getTestDataset(), new CMAnalyser&lt;Record&lt;FImage&gt;, String&gt;(CMAnalyser.Strategy.SINGLE));
				
Map&lt;Record&lt;FImage&gt;, ClassificationResult&lt;String&gt;&gt; guesses = eval.evaluate();
CMResult&lt;String&gt; result = eval.analyse(guesses);</pre>
            		<div class="sect1" title="12.1.&nbsp;Exercises">
               <div class="titlepage">
                  <div>
                     <div>
                        <h2 class="title" style="clear: both"><a name="classification101-exercises"></a>12.1.&nbsp;Exercises
                        </h2>
                     </div>
                  </div>
               </div>
               		  
               		  
               <div class="sect2" title="12.1.1.&nbsp;Exercise 1: Apply a Homogeneous Kernel Map">
                  <div class="titlepage">
                     <div>
                        <div>
                           <h3 class="title"><a name="exercise-1-apply-a-homogeneous-kernel-map"></a>12.1.1.&nbsp;Exercise 1: Apply a Homogeneous Kernel Map
                           </h3>
                        </div>
                     </div>
                  </div>
                  		    
                  		    
                  <p>
                     					A Homogeneous Kernel Map transforms data into a compact linear
                     					representation such that applying a linear classifier approximates, 
                     					to a high degree of accuracy, the application of a non-linear 
                     					classifier over the original data. Try using the
                     		      <code class="literal">HomogeneousKernelMap</code> class with a
                     		      <code class="literal">KernelType.Chi2</code> kernel and
                     		      <code class="literal">WindowType.Rectangular</code> window on top of the
                     		      <code class="literal">PHOWExtractor</code> feature extractor. What effect
                     		      does this have on performance?
                     		    
                  </p>
                  				
                  <div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;">
                     <table border="0" summary="Tip">
                        <tr>
                           <td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="images/tip.png"></td>
                           <th align="left">Tip</th>
                        </tr>
                        <tr>
                           <td align="left" valign="top">
                              			    
                              <p>
                                 			      Construct a <code class="literal">HomogeneousKernelMap</code> and use
                                 			      the <code class="literal">createWrappedExtractor()</code> method to create a
                                 			      new feature extractor around the <code class="literal">PHOWExtractor</code>
                                 			      that applies the map.
                                 			    
                              </p>
                              				
                           </td>
                        </tr>
                     </table>
                  </div>
                  		  
               </div>
               		  
               <div class="sect2" title="12.1.2.&nbsp;Exercise 2: Feature caching">
                  <div class="titlepage">
                     <div>
                        <div>
                           <h3 class="title"><a name="exercise-2-feature-caching"></a>12.1.2.&nbsp;Exercise 2: Feature caching
                           </h3>
                        </div>
                     </div>
                  </div>
                  		    
                  		    
                  <p>
                     		      The <code class="literal">DiskCachingFeatureExtractor</code> class can be
                     		      used to cache features extracted by a
                     		      <code class="literal">FeatureExtractor</code> to disk. It will generate and
                     		      save features if they don&#8217;t exist, or read from disk if they do.
                     		      Try to incorporate the
                     		      <code class="literal">DiskCachingFeatureExtractor</code> into your code.
                     		      You&#8217;ll also need to save the <code class="literal">HardAssigner</code> using
                     		      <code class="literal">IOUtils.writeToFile</code> and load it using
                     		      <code class="literal">IOUtils.readFromFile</code> because the features must
                     		      be kept with the same <code class="literal">HardAssigner</code> that created
                     		      them.
                     		    
                  </p>
                  		  
               </div>
               		  
               <div class="sect2" title="12.1.3.&nbsp;Exercise 3: The whole dataset">
                  <div class="titlepage">
                     <div>
                        <div>
                           <h3 class="title"><a name="exercise-3-the-whole-dataset"></a>12.1.3.&nbsp;Exercise 3: The whole dataset
                           </h3>
                        </div>
                     </div>
                  </div>
                  		    
                  		    
                  <p>
                     		      Try running the code over all the classes in the Caltech 101
                     		      dataset. Also try increasing the number of visual words to 600,
                     		      adding extra scales to the <code class="literal">PyramidDenseSIFT</code>
                     		      (try [4, 6, 8, 10] and reduce the step-size of the DenseSIFT to
                     		      3), and instead of using the
                     		      <code class="literal">BlockSpatialAggregator</code>, try the
                     		      <code class="literal">PyramidSpatialAggregator</code> with [2, 4] blocks.
                     		      What level of classifier performance does this achieve?
                     		    
                  </p>
                  		  
               </div>
               		
            </div>
            
         </div>
      </div>
      <div id="nav-sub-block" class="bottom"><a class="hide" name="nav-sub-a"></a><ul id="nav-sub">
            <li class="first"><a accesskey="p" href="pt06.html"><span>Prev : Machine Learning Fundamentals</span></a></li>
            <li><a accesskey="h" href="index.html"><span>Contents</span></a></li>
            <li class="last"><a accesskey="n" href="pt07.html"><span>Next : Facial Analysis</span></a></li>
         </ul>
      </div>
      <footer>
         <div class="container">
            <div class="row span12">Copyright &copy; 2011-2014 <a href="http://www.soton.ac.uk">The University of Southampton</a>. All Rights Reserved.
                 				
            </div>
            <p id="poweredBy" class="pull-right"><a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy"><img class="builtBy" alt="Built by Maven" src="./images/logos/maven-feather.png"></a></p>
            <div id="ohloh" class="pull-right"><script type="text/javascript" src="http://www.ohloh.net/p/openimaj/widgets/project_partner_badge.js"></script></div>
         </div>
      </footer>
   </body>
</html>